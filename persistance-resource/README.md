
# Running MosaicML training workloads on Vertex AI Training Persistent Resource

## Experimenting locally 

You can prepare and test a docker image that will be used with Vertex AI Training persistence resource on a user instance of Vertex Workbench. Make sure that your instance has the same GPU type as the persistance cluster. If you want to test FSDP locally allocate two GPUs to your instance.

### Build MosaicML docker image

Use the same docker image for both local testing and Vertex Training jobs.


```
PROJECT_ID=jk-mlops-dev
IMAGE_NAME=mosaicml-sandbox

docker build -t gcr.io/$PROJECT_ID/$IMAGE_NAME .
docker push gcr.io/$PROJECT_ID/$IMAGE_NAME
```


### Convert C4 dataset to StreamingDataset format

We will use the `convert_dataset_hf.py` script to convert the HF c4 dataset to the Mosaic StreamingDataset format and push the converted dataset to Google Cloud Storage. 

#### Create a GCS bucket

Create a GCS bucket in the same region where your persistant cluster will run.

```
export REGION=asia-southeast1
export BUCKET_NAME=gs://jk-asia-southeast1-staging

gsutil mb -l $REGION $BUCKET_NAME 
```

#### Run the conversion script

```
export C4_GCS_LOCATION=$BUCKET_NAME/c4

docker run -it --gpus all --rm \
--entrypoint /composer-python/python gcr.io/jk-mlops-dev/mosaicml-sandbox \
scripts/data_prep/convert_dataset_hf.py \
--dataset c4 --data_subset en \
--out_root $C4_GCS_LOCATION --splits train_small val_small \
--concat_tokens 2048 --tokenizer EleutherAI/gpt-neox-20b --eos_text '<|endoftext|>'
```

### Test a training run locally


```
RUN_ID="run-$(date +%s)"

docker run -it --gpus all --rm --shm-size=8g \
-v /home/jarekk/mds_cache:/mds_cache \
-v /home/jarekk/mosaic_runs:/runs \
--entrypoint composer gcr.io/jk-mlops-dev/mosaicml-sandbox \
scripts/train/train.py \
scripts/train/yamls/pretrain/mpt-125m.yaml \
run_name=$RUN_ID \
data_local=/mds_cache  \
data_remote=$C4_GCS_LOCATION  \
train_loader.dataset.split=train_small \
eval_loader.dataset.split=val_small \
max_duration=20ba \
eval_interval=10ba \
save_folder=/runs/checkpoints/{run_name} \
loggers.tensorboard='{log_dir: /runs/logs}'

```

## Running jobs on Vertex persistance resource

### Create a persistent resource

```
REGION=asia-southeast1
PROJECT_ID=jk-mlops-dev
PERSISTENT_RESOURCE_ID=jk-a100-cluster
DISPLAY_NAME=jk-a100-cluster
MACHINE_TYPE=a2-highgpu-2g
ACCELERATOR_TYPE=NVIDIA_TESLA_A100
ACCELERATOR_COUNT=2
REPLICA_COUNT=2
BOOT_DISK_TYPE=pd-ssd
BOOT_DISK_SIZE_GB=1000


gcloud beta ai persistent-resources create \
--persistent-resource-id=$PERSISTENT_RESOURCE_ID \
--display-name=$DISPLAY_NAME \
--project=$PROJECT_ID \
--region=$REGION \
--resource-pool-spec="replica-count=$REPLICA_COUNT,machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=$ACCELERATOR_COUNT,disk-type=$BOOT_DISK_TYPE,disk-size=$BOOT_DISK_SIZE_GB"

```

### List persistent resources

```
gcloud beta ai persistent-resources list --region=$REGION --project=$PROJECT_ID
```


### Run a job on a persistent resource

We will first run a training job for a 350M MPT model. The job parameters are in the `jobspec-350m.yaml`. The job is scheduled to run on a single node with two GPUs.

Before running the job adjust the `jobspec-350m.yaml` as required. To avoid overriding checkpoints and Tensorboard logs update the `args.run_name` parameter so each run has a unique ID. The `run_name` parameter will be used to create subfolders where checkpoints and Tensorboard logs are stored.



```
VERTEX_JOB_NAME="mosaicml-job-$(date +%s)"

gcloud beta ai custom-jobs create \
--region=$REGION \
--display-name=$VERTEX_JOB_NAME \
--persistent-resource-id $PERSISTENT_RESOURCE_ID \
--config=jobspec-350m.yaml
```


### Monitor the status of the job

You can monitor the status of the job using GCP Console or using the following gcloud command:

```
gcloud beta ai custom-jobs describe <YOUR JOB ID>
```


### Inspect the logs

You can inspect the logs using GCP Console or stream them using the following gcloud command:

```
gcloud beta ai custom-jobs stream-logs <YOUR JOB I\D>
```

### Inspect the persistent cluster

```
gcloud beta ai persistent-resources describe $PERSISTENT_RESOURCE_ID --region $REGION --project $PROJECT_ID
```

Notice that the `usedReplicaCount` is set to 1, meaning that one of the nodes is occupied by your job.

### Submit another job

If there are still unused nodes you can submit another job. In this example we will submit a job to train a 1B MPT model. The job spec is in `jobspec-1b.yaml`. Make sure to modify the `args.run_name` parameter.

```
VERTEX_JOB_NAME="mosaicml-job-$(date +%s)"

gcloud beta ai custom-jobs create \
--region=$REGION \
--display-name=$VERTEX_JOB_NAME \
--persistent-resource-id $PERSISTENT_RESOURCE_ID \
--config=jobspec-1b.yaml
```

### Inspect the artifacts created by the jobs

As configured in the job spec yaml files both the checkpoints and Tensorboard logs generated by the job are stored in the specified GCS locations.

Checkpoints are stored in the `save_model` location.
Tensorboard logs are stored in the `log_dir` location.

You can inspect these artifacts with the `gsutil` command.

### Analyze Tensorboard logs

You can use Vertex Tensorboard to review and analyze the training logs.

#### Create a Vertex Tensorboard instance

Your Tensorboard instance can be in a different region than your persistent cluster.


```
TENSORBOARD_REGION=us-central1
TENSORBOARD_NAME=jk-mosaicml-tb-log

gcloud ai tensorboards create \
--display-name $TENSORBOARD_NAME \
--project $PROJECT_ID \
--region $TENSORBOARD_REGION

```


#### Upload the logs 

You use the `tb-gcp-uploader` utility to upload the logs to your instance. The utility is included in the `google-cloud-aiplatform` package. To ensure that the utility is available (re)install the package.

```
pip install -U google-cloud-aiplatform[tensorboard]
```

 Note that you will need  a fully qualified name of your Tensorboard instance to configure `tb-gcp-uploader`. You can get it using the following command. This is a value in the `name:` field.

```
gcloud ai tensorboards list --project $PROJECT_ID --region $TENSORBOARD_REGION --filter="displayName=$TENSORBOARD_NAME"

```

To differentiate between different log groups in the same Tensorboard instance you upload them to an experiment.  You can upload multiple logs to the same experiment.

To upload the logs execute the following command:

```
TENSORBOARD_INSTANCE_ID=projects/895222332033/locations/us-central1/tensorboards/8170506090375544832
LOG_DIR=gs://jk-asia-southeast1-staging/mosaicml/tb_logs/350m-run-2
EXPERIMENT_NAME=mpt-runs

tb-gcp-uploader  \
--tensorboard_resource_name $TENSORBOARD_INSTANCE_ID \
--logdir=$LOG_DIR \
--experiment_name=$EXPERIMENT_NAME \
--one_shot 

```

After uploading you can analyze the results in Vertex AI Tensorboar UI.


### Delete a persistent resource

```
gcloud beta ai persistent-resources delete $PERSISTENT_RESOURCE_ID --region $REGION --project $PROJECT_ID
```


